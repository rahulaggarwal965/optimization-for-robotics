\documentclass[reqno]{amsart}

\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}  

  
\let\fullref\autoref
%
%  \autoref is very crude.  It uses counters to distinguish environments
%  so that if say {lemma} uses the {theorem} counter, then autrorefs
%  which should come out Lemma X.Y in fact come out Theorem X.Y.  To
%  correct this give each its own counter eg:
%                 \newtheorem{theorem}{Theorem}[section]
%                 \newtheorem{lemma}{Lemma}[section]
%  and then equate the counters by commands like:
%                 \makeatletter
%                   \let\c@lemma\c@theorem
%                  \makeatother
%
%  To work correctly the environment name must have a corrresponding 
%  \XXXautorefname defined.  The following command does the job:
%
\def\makeautorefname#1#2{\expandafter\def\csname#1autorefname\endcsname{#2}}
%
%  Some standard autorefnames.  If the environment name for an autoref 
%  you need is not listed below, add a similar line to your TeX file:
%  
%\makeautorefname{equation}{Equation}%
\def\equationautorefname~#1\null{(#1)\null}
\makeautorefname{footnote}{footnote}%
\makeautorefname{item}{item}%
\makeautorefname{figure}{Figure}%
\makeautorefname{table}{Table}%
\makeautorefname{part}{Part}%
\makeautorefname{appendix}{Appendix}%
\makeautorefname{chapter}{Chapter}%
\makeautorefname{section}{Section}%
\makeautorefname{subsection}{Section}%
\makeautorefname{subsubsection}{Section}%
\makeautorefname{theorem}{Theorem}%
\makeautorefname{thm}{Theorem}%
\makeautorefname{cor}{Corollary}%
\makeautorefname{lem}{Lemma}%
\makeautorefname{prop}{Proposition}%
\makeautorefname{pro}{Property}
\makeautorefname{conj}{Conjecture}%
\makeautorefname{defn}{Definition}%
\makeautorefname{notn}{Notation}
\makeautorefname{notns}{Notations}
\makeautorefname{rem}{Remark}%
\makeautorefname{quest}{Question}%
\makeautorefname{exmp}{Example}%
\makeautorefname{ax}{Axiom}%
\makeautorefname{claim}{Claim}%
\makeautorefname{ass}{Assumption}%
\makeautorefname{asss}{Assumptions}%
\makeautorefname{con}{Construction}%
\makeautorefname{prob}{Problem}%
\makeautorefname{warn}{Warning}%
\makeautorefname{obs}{Observation}%
\makeautorefname{conv}{Convention}%


%
%                  *** End of hyperref stuff ***

%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{conj}{Conjecture}[section]
%\newtheorem{ass}{Assumption}[section]
%\newtheorem{asses}{Assumptions}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{ass}{Assumption}[section]
\newtheorem{asss}{Assumptions}[section]
\newtheorem{ax}{Axiom}[section]
\newtheorem{con}{Construction}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{notn}{Notation}[section]
\newtheorem{notns}{Notations}[section]
\newtheorem{pro}{Property}[section]
\newtheorem{quest}{Question}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{warn}{Warning}[section]
\newtheorem{sch}{Scholium}[section]
\newtheorem{obs}{Observation}[section]
\newtheorem{conv}{Convention}[section]

%%%% hack to get fullref working correctly
\makeatletter
\let\c@obs=\c@thm
\let\c@cor=\c@thm
\let\c@prop=\c@thm
\let\c@lem=\c@thm
\let\c@prob=\c@thm
\let\c@con=\c@thm
\let\c@conj=\c@thm
\let\c@defn=\c@thm
\let\c@notn=\c@thm
\let\c@notns=\c@thm
\let\c@exmp=\c@thm
\let\c@ax=\c@thm
\let\c@pro=\c@thm
\let\c@ass=\c@thm
\let\c@warn=\c@thm
\let\c@rem=\c@thm
\let\c@sch=\c@thm
\let\c@equation\c@thm
\numberwithin{equation}{section}
\makeatother

\bibliographystyle{plain}

%--------Meta Data: Fill in your info------
\title{Optimization on the $SO(3)$ and $SE(3)$ Manifolds}

\author{Rahul Aggarwal}

\date{November 18, 2022}

\begin{document}

\begin{abstract}

\end{abstract}

\maketitle

\tableofcontents

\section{Smooth Manifolds}

Before we talk about manifolds, let alone differentiating on manifolds, we need to lock down both of these concepts in a formal sense.
A smooth or differentiable manifold is one that looks locally Euclidean. Intuitively, this should mean that every point on said manifold has some open neighborhood around it that can be ``flattened'' to look like a plane.

\begin{defn}
    Let $X$ be a topological space. $X$ is \textbf{locally Euclidean} if there is exists some $n \in N$ such that every point $p \in X$ has an open neighborhood $U \subseteq X$ which is homeomorphic to an open subset of $\mathbb{R}^n$.
\end{defn}

Manifolds are distinguished from topological spaces by the fact that they are locally Euclidean, albeit with the additional properties of being Hausdorff and second countable (which are out of scope of this paper).

However, as of now, we have not attributed any structure, or \textit{differentiability} to the topological manifold. We need to add to our notion of being locally Euclidean the idea of smoothness. To do so, we need to be able to think about a differentiable homeomorphism.

\begin{defn}
    A smooth map $f: M \to N$ is a \textbf{diffeomorphism} if it is a bijection and its inverse $f^{-1}: N \to M$ is smooth as well.
\end{defn}

It is useful to think of a topological manifold as ``consisting'' of a countable collection of open subsets $U_i$ corresponding to maps $\phi_i$ that carry points in $U_i$ onto an open subset of $\mathbb{R}^n$. Formally each tuple $\{U_i, \phi_i\}$ is called a \textbf{coordinate chart}, and the collection of these charts is called an \textbf{atlas}. Using this, we can now define a smooth manifold.

\begin{defn}
    Let $X$ be an $n$-dimensional topological manifold. $X$ is smooth if for every coordinate chart $\{U_i, \phi_i\}$ in its atlas, $\phi_i$ is smooth and if $U_i$ and $U_j$ overlap, then the transition map $\phi_{ij} = \phi_i \circ \phi_j^{-1}$ is a diffeomorphism.
\end{defn}

The easiest way to think about smooth manifolds is as a collection of ``patches'' of Euclidean space.

\begin{rem}
    We refer to $\phi_i^{-1}: \mathbb{R}^n \to U_i$ as a local parameterization of $U_i$, taking \textbf{local coordinates} in $\mathbb{R}^n$ onto the manifold. This is useful when thinking about functions acting on the manifold.
\end{rem}

\section{Lie Groups}

A \textbf{Lie group} is a group that is also a smooth manifold, thereby possessing a smooth group operation.

\section{Optimization for Robotics}

Suppose we had a robotic platform for which we wanted an accurate estimate of the orientation $R \in SO(3)$, which we know to have three degrees of freedom. One common and often used way to refer to the orientation are the Euler angles: roll, pitch, and yaw. However, this representation of orientation suffers from a condition called gimbal lock, which rises from the fact that they do not form a smooth chart on all of $SO(3)$. Instead, we want to optimize on the manifold itself.

Suppose we have some non-linear measurement function:
\[h: SO(3) \to \mathbb{R}^n : R \mapsto z\]
To try and estimate the unknown orientation $R$, we want to minimize a non-linear least squares error criterion
\begin{equation}\label{ObjectiveFunction}
R^* = \underset{R}{\arg\min}\|h(R) - z\|_\Sigma^2
\end{equation}
where $\|e\|_\Sigma^2 \triangleq e^T\Sigma^{-1}e$ is the squared Mahalanobis distance with covariance $\Sigma$. 

\begin{rem}
    The Mahalanobis distance is the multi-dimensional generalization of the measure of how many standard deviations a point $p$ is from the mean of a distribution $D$.
\end{rem}

To minimize (\ref{ObjectiveFunction}), we need to know how the non-linear function $h$ behaves in the neighborhood of a linearization point $a$. Roughly speaking, we want to define a Jacobian matrix $H_a$ such that
\begin{equation}
    h(a \oplus \epsilon) \approx h(a) + H_a\epsilon
\end{equation}

with $\epsilon$ being defined in the local coordinates of $SO(3)$. Using this approximation, we can now minimize (\ref{ObjectiveFunction}) with respect to $\delta{x}$:

\begin{equation}\label{NewObjectiveFunction}
    \epsilon^* = \underset{\epsilon}{\arg\min}\|h(a) + H_a\epsilon - z\|_\Sigma^2
\end{equation}

We solve this by setting the derivative (\ref{NewObjectiveFunction}) to zero, yielding
\begin{equation*}
    H_a^TH_a\epsilon = H_a^T(z - h(a))
\end{equation*}

Thus, starting with an initial estimate $R_0$, we are able to iteratively find better estimates for $R$, thereby improving our estimate of the robot's orientation. 

\section{Optimization on Lie Groups}

\section*{Acknowledgments}  You should thank anyone who deserves thanks, and for sure you should
thank your mentor.   ``It is a pleasure to thank my mentor, 
his/her name, for ....  ".   Or add anyone else, for example ``I thank [another participant] for helping 
me understand [something or other]"

\begin{thebibliography}{9}

\bibitem{ams} http://www.ams.org/publications/authors/tex/amslatex

\bibitem{amsshort}
Michael Downes.
Short Math Guide for \LaTeX.
http://tex.loria.fr/general/downes-short-math-guide.pdf

\bibitem{May}
J. P. May.
A Concise Course in Algebraic Topology.
University of Chicago Press. 1999. 

\bibitem{notsoshort}
Tobias Oekiter, Hubert Partl, Irene Hyna and Elisabeth Schlegl.
The Not So Short Introduction to \LaTeX 2e.
https://tobi.oetiker.ch/lshort/lshort.pdf

\end{thebibliography}

\end{document}

